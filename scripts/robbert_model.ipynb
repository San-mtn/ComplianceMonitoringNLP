{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RobBERT Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing necessary libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import Counter, defaultdict\n",
    "from datasets import load_metric\n",
    "from transformers import RobertaTokenizerFast, RobertaTokenizer, RobertaForTokenClassification, AdamW, get_scheduler, pipeline, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from fuzzywuzzy import fuzz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('final_data.csv')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('pdelobelle/robbert-v2-dutch-ner')\n",
    "model = RobertaForTokenClassification.from_pretrained('pdelobelle/robbert-v2-dutch-ner', return_dict=True)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print(\"RobBERT model loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=0.3, random_state=42)\n",
    "\n",
    "print(\"Train set size:\", len(train))\n",
    "print(\"Test set size:\", len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Organizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_predict(text):\n",
    "    # tokenize the text\n",
    "    encoded_input = tokenizer(text, truncation=True, padding='max_length', max_length=512, return_tensors=\"pt\")\n",
    "    \n",
    "    # move tensors to the same device as model\n",
    "    encoded_input = {key: val.to(model.device) for key, val in encoded_input.items()}\n",
    "    \n",
    "    # predict using the model\n",
    "    with torch.no_grad():\n",
    "        output = model(**encoded_input)\n",
    "    \n",
    "    return output, encoded_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_select_most_common_org(text):\n",
    "    output, encoded_input = tokenize_and_predict(text)\n",
    "    predictions = output.logits.argmax(dim=-1).squeeze().tolist()  # Get the predicted class ID for each token\n",
    "    \n",
    "    tokens = tokenizer.convert_ids_to_tokens(encoded_input['input_ids'].squeeze().tolist())\n",
    "    \n",
    "    entities = []\n",
    "    current_entity = []\n",
    "    for token, pred in zip(tokens, predictions):\n",
    "        entity_label = model.config.id2label[pred]\n",
    "        if entity_label:\n",
    "            if entity_label.endswith('ORG'):\n",
    "                # Remove RoBERTa's space token\n",
    "                current_entity.append(token.replace('Ġ', ' '))\n",
    "        else:\n",
    "            if current_entity:\n",
    "                entities.append(''.join(current_entity).strip())\n",
    "                current_entity = []\n",
    "    \n",
    "    if current_entity:\n",
    "        entities.append(''.join(current_entity).strip())\n",
    "    \n",
    "    # Count the entities found in this text\n",
    "    entity_counter = Counter(entities)\n",
    "    # Select the most common one, or None if no entities are found\n",
    "    most_common_entity = str(entity_counter.most_common(1)[0][0]) if entity_counter else 'No prediction'\n",
    "    \n",
    "    return most_common_entity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting Organizations: 100%|██████████| 269/269 [04:53<00:00,  1.09s/it]\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas(desc=\"Predicting Organizations\")\n",
    "\n",
    "# Apply entity extraction to the cleaned text column with progress tracking\n",
    "test['Predicted Organization'] = test['Cleaned Text'].progress_apply(extract_and_select_most_common_org)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 2.23%\n"
     ]
    }
   ],
   "source": [
    "def calculate_pretrained_accuracy(test):\n",
    "    predicted_orgs = list(test['Predicted Organization'])\n",
    "    true_orgs = list(test['True Organization'])\n",
    "\n",
    "    correct_predictions = 0\n",
    "    for pred, truth in zip(predicted_orgs, true_orgs):\n",
    "        # normalize the data to lower case to ignore case sensitivity\n",
    "        pred = str(pred)\n",
    "        truth = str(truth)\n",
    "        pred = pred.lower().strip()\n",
    "        truth = truth.lower().strip()\n",
    "\n",
    "        # check for exact or partial match\n",
    "        if pred == truth or pred in truth or truth in pred:\n",
    "            correct_predictions += 1\n",
    "\n",
    "    total_predictions = len(predicted_orgs)\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    return f\"Accuracy: {accuracy * 100:.2f}%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculated_pretrained_fuzzy_accuracy(test):\n",
    "    predicted_orgs = list(test['Predicted Organization'])\n",
    "    true_orgs = list(test['True Organization'])\n",
    "    \n",
    "    correct_predictions = 0\n",
    "    for pred, truth in zip(predicted_orgs, true_orgs):\n",
    "        # normalize the data to lower case to ignore case sensitivity\n",
    "        pred = str(pred)\n",
    "        truth = str(truth)\n",
    "        pred = pred.lower().strip()\n",
    "        truth = truth.lower().strip()\n",
    "\n",
    "        # check for exact, partial, or fuzzy match\n",
    "        if pred != 'no prediction':\n",
    "            if fuzz.partial_ratio(pred, truth) >= 80:\n",
    "                correct_predictions += 1\n",
    "\n",
    "    total_predictions = len(predicted_orgs)\n",
    "    accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "    return f\"Accuracy: {accuracy * 100:.2f}%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_pretrained_accuracy(test)\n",
    "calculated_pretrained_fuzzy_accuracy(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision & recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_orgs = list(test['Predicted Organization'])\n",
    "true_orgs = list(test['True Organization'])\n",
    "prediction_presence = []\n",
    "\n",
    "for pred, truth in zip(predicted_orgs, true_orgs):\n",
    "    pred = str(pred)\n",
    "    truth = str(truth)\n",
    "    pred = pred.lower().strip()\n",
    "    truth = truth.lower().strip()\n",
    "\n",
    "    # check if there is any prediction\n",
    "    if pred != 'no prediction':\n",
    "        prediction_presence.append(1)\n",
    "    else:\n",
    "        prediction_presence.append(0)\n",
    "\n",
    "print(\"Prediction Presence:\", prediction_presence)\n",
    "print(f\"Presence Array Length: {len(prediction_presence)}\")\n",
    "\n",
    "robbert_preds = prediction_presence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list with 1 if organization is present in the text, 0 otherwise\n",
    "# see calculation of actuals in current_method notebook\n",
    "actuals = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_precision_recall(predictions, actuals):\n",
    "    TP = sum(1 for actual, pred in zip(actuals, predictions) if actual == 1 and pred == 1)\n",
    "    FP = sum(1 for actual, pred in zip(actuals, predictions) if actual == 0 and pred == 1)\n",
    "    FN = sum(1 for actual, pred in zip(actuals, predictions) if actual == 1 and pred == 0)\n",
    "    \n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "\n",
    "    return precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_r, recall_r = calculate_precision_recall(robbert_preds, actuals)\n",
    "\n",
    "print(f\"Precision RobBERT pretrained: {precision_r:.2f}\")\n",
    "print(f\"Recall RobBERT pretrained: {recall_r:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuned Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load tokenizer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer and model\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained('pdelobelle/robbert-v2-dutch-ner')\n",
    "model = RobertaForTokenClassification.from_pretrained('pdelobelle/robbert-v2-dutch-ner', return_dict=True)\n",
    "\n",
    "# define label mapping\n",
    "label_list = [\"O\", \"B-ORG\", \"I-ORG\"]\n",
    "label_map = {label: i for i, label in enumerate(label_list)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to tokenize and align labels\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples['Cleaned Text'], truncation=True, padding='max_length', max_length=512, return_offsets_mapping=True)\n",
    "    labels = []\n",
    "\n",
    "    for i, label in enumerate(examples['True Organization']):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        label_ids = [-100] * len(word_ids)\n",
    "\n",
    "        org_positions = re.finditer(re.escape(label), examples['Cleaned Text'][i])\n",
    "        for match in org_positions:\n",
    "            start, end = match.start(), match.end()\n",
    "            for idx, word_id in enumerate(word_ids):\n",
    "                if word_id is None:\n",
    "                    continue\n",
    "                if tokenized_inputs['offset_mapping'][i][idx][0] == start:\n",
    "                    label_ids[idx] = label_map[\"B-ORG\"]\n",
    "                elif start < tokenized_inputs['offset_mapping'][i][idx][0] < end:\n",
    "                    label_ids[idx] = label_map[\"I-ORG\"]\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_pandas(train)\n",
    "test_dataset = Dataset.from_pandas(test)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_and_align_labels, batched=True, remove_columns=train_dataset.column_names, desc=\"Tokenizing train dataset\")\n",
    "test_dataset = test_dataset.map(tokenize_and_align_labels, batched=True, remove_columns=test_dataset.column_names, desc=\"Tokenizing test dataset\")\n",
    "\n",
    "datasets = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'test': test_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,  \n",
    "    per_device_train_batch_size=16,  \n",
    "    per_device_eval_batch_size=16,  \n",
    "    num_train_epochs=10,  \n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# define the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=datasets['train'],\n",
    "    eval_dataset=datasets['test'],\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, label_ids, metrics = trainer.predict(datasets['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to align predictions with true labels\n",
    "def align_predictions(predictions, label_ids):\n",
    "    preds = np.argmax(predictions, axis=2)\n",
    "\n",
    "    batch_size, seq_len = preds.shape\n",
    "\n",
    "    out_label_list = [[] for _ in range(batch_size)]\n",
    "    preds_list = [[] for _ in range(batch_size)]\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        for j in range(seq_len):\n",
    "            if label_ids[i, j] != -100:\n",
    "                out_label_list[i].append(label_ids[i][j])\n",
    "                preds_list[i].append(preds[i][j])\n",
    "\n",
    "    return preds_list, out_label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map labels to tag names\n",
    "def get_labels(preds_list, out_label_list):\n",
    "    preds_tags = []\n",
    "    true_tags = []\n",
    "    for preds, true in zip(preds_list, out_label_list):\n",
    "        preds_tags.append([label_list[p] if p < len(label_list) else \"O\" for p in preds])\n",
    "        true_tags.append([label_list[t] if t < len(label_list) else \"O\" for t in true])\n",
    "    return preds_tags, true_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_list, out_label_list = align_predictions(predictions, label_ids)\n",
    "\n",
    "preds_tags, true_tags = get_labels(preds_list, out_label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract entities from tags\n",
    "def extract_entities(text, tags):\n",
    "    entities = []\n",
    "    entity = \"\"\n",
    "    for word, tag in zip(text.split(), tags):\n",
    "        if tag == \"B-ORG\":\n",
    "            if entity:\n",
    "                entities.append(entity)\n",
    "            entity = word\n",
    "        elif tag == \"I-ORG\" and entity:\n",
    "            entity += \" \" + word\n",
    "        else:\n",
    "            if entity:\n",
    "                entities.append(entity)\n",
    "                entity = \"\"\n",
    "    if entity:\n",
    "        entities.append(entity)\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(test)\n",
    "    predicted_entities = [extract_entities(text, tags) for text, tags in zip(test['Cleaned Text'], preds_tags)]\n",
    "    true_entities = [[org] for org in test['True Organization']]\n",
    "    correct_predictions = 0\n",
    "    total_predictions = len(true_entities)\n",
    "\n",
    "    for pred, true in zip(predicted_entities, true_entities):\n",
    "        pred_str = ' '.join(pred)\n",
    "        true_str = ' '.join(true)\n",
    "        if pred_str != '':\n",
    "            if pred_str in true_str or true_str in pred_str or pred_str == true_str:\n",
    "                correct_predictions += 1\n",
    "\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    return f\"Accuracy: {accuracy * 100:.2f}%\"\n",
    "\n",
    "calculate_accuracy(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fuzzy_accuracy(test):\n",
    "    predicted_entities = [extract_entities(text, tags) for text, tags in zip(test['Text'], preds_tags)]\n",
    "    true_entities = [[org] for org in test['True Organization']]\n",
    "\n",
    "    # combine predicted entities into a single string for comparison\n",
    "    combined_predicted_entities = [' '.join(pred).lower() for pred in predicted_entities]\n",
    "\n",
    "    # compare predicted entities to true entities\n",
    "    correct_predictions = 0\n",
    "    total_predictions = len(true_entities)\n",
    "    prediction_presence = []\n",
    "\n",
    "    for combined_pred, true in zip(combined_predicted_entities, true_entities):\n",
    "        true_org = true[0].lower()\n",
    "        if combined_pred:\n",
    "            # fuzzy match score for the combined string\n",
    "            match_score = fuzz.partial_ratio(combined_pred, true_org)\n",
    "\n",
    "            # consider a prediction correct if the match score is above a certain threshold\n",
    "            if match_score >= 80:  # You can adjust this threshold as needed\n",
    "                correct_predictions += 1\n",
    "\n",
    "    # individual entity matching\n",
    "    for pred, true in zip(predicted_entities, true_entities):\n",
    "        true_org = true[0]\n",
    "        if pred:\n",
    "            match_scores = [fuzz.partial_ratio(pred_entity.lower(), true_org.lower()) for pred_entity in pred]\n",
    "            best_match_score = max(match_scores) if match_scores else 0\n",
    "            if best_match_score >= 80: \n",
    "                correct_predictions += 1\n",
    "\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    return f\"Accuracy: {accuracy * 100:.2f}%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_fuzzy_accuracy(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction presence for precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_presence(test):\n",
    "    predicted_entities = [extract_entities(text, tags) for text, tags in zip(test['Cleaned Text'], preds_tags)]\n",
    "    true_entities = [[org] for org in test['True Organization']]\n",
    "    prediction_presence = []\n",
    "\n",
    "    for pred, true in zip(predicted_entities, true_entities):\n",
    "        pred_str = ' '.join(pred)\n",
    "        true_str = ' '.join(true)\n",
    "        if pred_str != '':\n",
    "            prediction_presence.append(1)\n",
    "        else:\n",
    "            prediction_presence.append(0)\n",
    "\n",
    "    return prediction_presence\n",
    "\n",
    "robbert_preds = prediction_presence(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_r, recall_r = calculate_precision_recall(robbert_preds, actuals)\n",
    "\n",
    "print(f\"Precision RobBERT finetuned: {precision_r:.2f}\")\n",
    "print(f\"Recall RobBERT finetuned: {recall_r:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seconddata = pd.read_csv('final_seconddata.csv')\n",
    "\n",
    "test_dataset = Dataset.from_pandas(seconddata)\n",
    "secondtest = test_dataset.map(tokenize_and_align_labels, batched=True, remove_columns=test_dataset.column_names, desc=\"Tokenizing test dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, label_ids, metrics = trainer.predict(secondtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_list, out_label_list = align_predictions(predictions, label_ids)\n",
    "\n",
    "preds_tags, true_tags = get_labels(preds_list, out_label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_accuracy(secondtest)\n",
    "calculate_fuzzy_accuracy(secondtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document-level accuracy\n",
    "For the comparison between models / t-tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_org_dict = test.set_index('Cleaned Text')['True Organization'].to_dict()\n",
    "doc_acc = []\n",
    "total_predictions = len(predictions)\n",
    "\n",
    "# iterate over test data and check if the predicted organization name matches the true organization\n",
    "for idx, row in test.iterrows():\n",
    "    true_org = true_org_dict[row['Cleaned Text']]\n",
    "    pred_org = predictions[idx]\n",
    "\n",
    "    # normalize the text for comparison\n",
    "    true_org = true_org.lower().strip()\n",
    "    pred_org = pred_org.lower().strip()\n",
    "\n",
    "    # check if the organization name matches the true organization\n",
    "    if true_org == pred_org or true_org in pred_org or pred_org in true_org:\n",
    "        doc_acc.append(1)\n",
    "    else:\n",
    "        doc_acc.append(0)\n",
    "\n",
    "print(doc_acc)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
